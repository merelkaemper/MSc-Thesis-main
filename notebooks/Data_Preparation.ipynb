{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook all the datatransformation steps are excuted and described. This file should be run first to get the monthly_trajectories.csv file from the monthly_trajectories_nl dataframe. The csv file will be input for the scripts (starting with 0_Feature_Engineering.py). In the Exploratory_Data_Analysis.ipynb more details on the different dataframes can be found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/merelkamper/anaconda3/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load service data\n",
    "folder_path = '/Users/merelkamper/Documents/MSc Data Science/Thesis/train_rides'\n",
    "files = os.listdir(folder_path)\n",
    "dfs=[]\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine different files                \n",
    "all_train_services = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data to NS data and trains\n",
    "ns_data = all_train_services[all_train_services['Service:Company'].str.lower().str.contains('ns')]\n",
    "ns_train_data = ns_data[ns_data['Service:Type'].isin(['Intercity', 'Sprinter', 'Intercity direct', 'Eurostar', 'Thalys', 'Extra trein', \n",
    "                                                      'ICE International', 'Int. Trein', 'Speciale Trein', 'Nightjet'])]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115266904\n",
      "78403382\n"
     ]
    }
   ],
   "source": [
    "print(len(all_train_services))\n",
    "print(len(ns_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TRANSOFRMATION STEPS FIRST\n",
    "\n",
    "# Filter on rides in NL\n",
    "# Load station data\n",
    "stations_folder = 'stations_data'\n",
    "station_files = ['stations-2019-07.csv', 'stations-2020-01.csv', 'stations-2022-01.csv', 'stations-2023-09.csv']\n",
    "\n",
    "# Initialize an empty DataFrame for station data\n",
    "stations_data = pd.DataFrame()\n",
    "\n",
    "# Read and concatenate all station files\n",
    "for file in station_files:\n",
    "    file_path = os.path.join(stations_folder, file)\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "    stations_data = pd.concat([stations_data, temp_df], ignore_index=True)\n",
    "\n",
    "# Ensure there are no duplicate station entries\n",
    "stations_data = stations_data.drop_duplicates(subset=['name_long'])\n",
    "\n",
    "# Merge final_df with stations_data to get country information for source stations\n",
    "monthly_trajectories = monthly_trajectories.merge(stations_data[['name_long', 'country']], how='left', left_on='source', right_on='name_long', suffixes=('', '_source'))\n",
    "monthly_trajectories = monthly_trajectories.rename(columns={'country': 'source_country'})\n",
    "monthly_trajectories = monthly_trajectories.drop(columns=['name_long'])\n",
    "\n",
    "# Merge final_df with stations_data to get country information for target stations\n",
    "monthly_trajectories = monthly_trajectories.merge(stations_data[['name_long', 'country']], how='left', left_on='target', right_on='name_long', suffixes=('', '_target'))\n",
    "monthly_trajectories = monthly_trajectories.rename(columns={'country': 'target_country'})\n",
    "monthly_trajectories = monthly_trajectories.drop(columns=['name_long'])\n",
    "\n",
    "# Ensure there are no duplicate columns\n",
    "monthly_trajectories = monthly_trajectories.loc[:,~monthly_trajectories.columns.duplicated()]\n",
    "\n",
    "# Filter to keep only rides where both source and target stations are in the Netherlands\n",
    "monthly_trajectories_nl = monthly_trajectories[(monthly_trajectories['source_country'] == 'NL') & (monthly_trajectories['target_country'] == 'NL')]\n",
    "monthly_trajectories_nl = monthly_trajectories_nl.drop(columns=['source_country','target_country'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations on the original dataset to group the trajectories per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/j8k6m66124x5lcy646zpdz680000gn/T/ipykernel_2187/3674219698.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  definite_df['Arrival Delay of Last Stop (min)'].fillna(-1, inplace=True)\n",
      "/var/folders/3g/j8k6m66124x5lcy646zpdz680000gn/T/ipykernel_2187/3674219698.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  definite_df['Last Arrival Cancelled'].fillna(True, inplace=True)\n",
      "/var/folders/3g/j8k6m66124x5lcy646zpdz680000gn/T/ipykernel_2187/3674219698.py:41: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  definite_df['Last Arrival Cancelled'].fillna(True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# DATA TRANSFORMATION 1\n",
    "grouped = ns_train_data.groupby('Service:RDT-ID')\n",
    "\n",
    "rdt_ids = grouped['Service:RDT-ID'].unique()\n",
    "trajectories = ns_train_data.groupby('Service:RDT-ID')['Stop:Station name'].agg(['first', 'last']).agg(' - '.join, axis=1)\n",
    "dates = grouped['Service:Date'].first()\n",
    "day_of_week = pd.to_datetime(dates).dt.day_name()\n",
    "max_delays = grouped['Service:Maximum delay'].first()\n",
    "arrival_delays_last_stop = grouped['Stop:Arrival delay'].last()\n",
    "planned_stops = np.maximum(grouped.size() - 2, 0)  # Subtract 2 for departure and arrival stops\n",
    "cancelled_arrivals = grouped['Stop:Arrival cancelled'].sum()\n",
    "cancelled_departures = grouped['Stop:Departure cancelled'].sum()\n",
    "delayed_arrivals = (ns_train_data['Stop:Arrival delay'] > 0).groupby(ns_train_data['Service:RDT-ID']).sum()\n",
    "delayed_departures = (ns_train_data['Stop:Departure delay'] > 0).groupby(ns_train_data['Service:RDT-ID']).sum()\n",
    "partly_cancelled = grouped['Service:Partly cancelled'].any()\n",
    "completely_cancelled = grouped['Service:Completely cancelled'].all()\n",
    "last_stop_cancelled = grouped.last()['Stop:Arrival cancelled']\n",
    "\n",
    "definite_df = pd.DataFrame({\n",
    "    'RDT-ID': rdt_ids,\n",
    "    'Trajectory': trajectories,\n",
    "    'Date': dates,\n",
    "    'Day of the Week': day_of_week,\n",
    "    'Maximum Delay': max_delays,\n",
    "    'Arrival Delay of Last Stop (min)': arrival_delays_last_stop,\n",
    "    'Nr. of Planned Stops': planned_stops,\n",
    "    'Nr. of Cancelled Arrivals': cancelled_arrivals,\n",
    "    'Nr. of Cancelled Departures': cancelled_departures,\n",
    "    'Nr. of Delayed Arrivals': delayed_arrivals,\n",
    "    'Nr. of Delayed Departures': delayed_departures,\n",
    "    'Partly Cancelled': partly_cancelled,\n",
    "    'Completely Cancelled': completely_cancelled,\n",
    "    'Last Arrival Cancelled': last_stop_cancelled\n",
    "})\n",
    "\n",
    "# Handling of the 250 NaN values in \"Arrival Delay of Last Stop (min)\"\" and None values in \"Last Arrival Cancelled\"\n",
    "# Set 'Arrival Delay of Last Stop (min)' to -1 for unfinished trajectories\n",
    "definite_df['Arrival Delay of Last Stop (min)'].fillna(-1, inplace=True)\n",
    "\n",
    "# Set 'Last Arrival Cancelled' to True for these cases as they represent cancellations\n",
    "definite_df['Last Arrival Cancelled'].fillna(True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/j8k6m66124x5lcy646zpdz680000gn/T/ipykernel_2187/3437742489.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  trajectories_per_day['Final arrival delay'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# DATA TRANSFORMATION 2\n",
    "definite_df['Date'] = pd.to_datetime(definite_df['Date'])\n",
    "definite_df[['source', 'target']] = definite_df['Trajectory'].str.split(' - ', expand=True)\n",
    "\n",
    "definite_df['source'] = definite_df['source'].replace({'Amersfoort': 'Amersfoort Centraal', 'Eindhoven': 'Eindhoven Centraal'})\n",
    "definite_df['target'] = definite_df['target'].replace({'Amersfoort': 'Amersfoort Centraal', 'Eindhoven': 'Eindhoven Centraal'})\n",
    "\n",
    "rides_performed = definite_df.dropna(subset=['Arrival Delay of Last Stop (min)']).groupby(['Date','source', 'target']).size().reset_index(name='Rides planned')\n",
    "delayed_arrivals = definite_df[definite_df['Arrival Delay of Last Stop (min)'] > 0.0].groupby(['Date', 'source', 'target']).size().reset_index(name='Final arrival delay')\n",
    "arrival_canceled = definite_df.groupby(['Date', 'source', 'target'])['Last Arrival Cancelled'].sum().reset_index(name='Final arrival cancelled')\n",
    "trajectory_canceled = definite_df.groupby(['Date', 'source', 'target'])['Completely Cancelled'].sum().reset_index(name='Completely cancelled')\n",
    "intermediate_delays = definite_df[definite_df['Nr. of Delayed Arrivals'] > 0].groupby(['Date', 'source', 'target']).size().reset_index(name='Intermediate arrival delays')\n",
    "\n",
    "trajectories_per_day = rides_performed.merge(delayed_arrivals, on=['Date', 'source', 'target'], how='left')\n",
    "trajectories_per_day = trajectories_per_day.merge(arrival_canceled, on=['Date', 'source', 'target'], how='left')\n",
    "trajectories_per_day = trajectories_per_day.merge(trajectory_canceled, on=['Date', 'source', 'target'], how='left')\n",
    "trajectories_per_day = trajectories_per_day.merge(intermediate_delays, on=['Date', 'source', 'target'], how='left')\n",
    "\n",
    "trajectories_per_day['Final arrival delay'].fillna(0, inplace=True)\n",
    "trajectories_per_day['Final arrival delay'] = trajectories_per_day['Final arrival delay'].astype(int)\n",
    "trajectories_per_day['Final arrival cancelled'] = trajectories_per_day['Final arrival cancelled'].replace(False, 0).astype(int)\n",
    "trajectories_per_day['Intermediate arrival delays'] = trajectories_per_day['Intermediate arrival delays'].fillna(0).astype(int)\n",
    "\n",
    "# Remove rows where source is the same as target\n",
    "trajectories_per_day = trajectories_per_day[trajectories_per_day['source'] != trajectories_per_day['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39942\n",
      "30416\n"
     ]
    }
   ],
   "source": [
    "# DATA TRANSFORMATION 3\n",
    "# Convert the Date column to datetime format and create a YearMonth column\n",
    "trajectories_per_day['Date'] = pd.to_datetime(trajectories_per_day['Date'])\n",
    "trajectories_per_day['YearMonth'] = trajectories_per_day['Date'].dt.to_period('M')\n",
    "\n",
    "# Perform monthly aggregation\n",
    "monthly_trajectories = trajectories_per_day.groupby(['YearMonth', 'source', 'target'], as_index=False).agg({\n",
    "    'Rides planned': 'sum',\n",
    "    'Final arrival delay': 'sum',\n",
    "    'Final arrival cancelled': 'sum',\n",
    "    'Completely cancelled': 'sum',\n",
    "    'Intermediate arrival delays': 'sum'\n",
    "})\n",
    "\n",
    "# Filter out trajectories with fewer than 4 rides planned per month\n",
    "min_rides_threshold = 4\n",
    "print(len(monthly_trajectories))\n",
    "monthly_trajectories = monthly_trajectories[monthly_trajectories['Rides planned'] >= min_rides_threshold]\n",
    "print(len(monthly_trajectories))\n",
    "\n",
    "# Recalculate the proportion delayed after monthly aggregation\n",
    "monthly_trajectories['Proportion delayed'] = monthly_trajectories.apply(\n",
    "    lambda row: 0 if row['Final arrival delay'] == 0 else \n",
    "    row['Final arrival delay'] / (row['Rides planned'] - row['Completely cancelled']) \n",
    "    if (row['Rides planned'] - row['Completely cancelled']) > 0 else 0, axis=1)\n",
    "\n",
    "# Calculate the 90th percentile threshold\n",
    "percentile_50 = monthly_trajectories['Proportion delayed'].quantile(0.50)\n",
    "\n",
    "# Add a column to indicate if the delay is significant based on the threshold\n",
    "monthly_trajectories['Significant Delay'] = monthly_trajectories['Proportion delayed'] > percentile_50 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the threshold is for a trajectorie to be significantly delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21014492753623187"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 50th percentile creates a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True values in data 14392, Number of False values in data 14165\n"
     ]
    }
   ],
   "source": [
    "numberTrue = monthly_trajectories_nl[monthly_trajectories_nl['Significant Delay'] == True]\n",
    "numberFalse = monthly_trajectories_nl[monthly_trajectories_nl['Significant Delay'] == False]\n",
    "print(f\"Number of True values in data {len(numberTrue)}, Number of False values in data {len(numberFalse)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the original dataset is 115266904\n",
      "The number of rows in the cleaned dataset is 75620094\n",
      "The number of rows in the first transformed dataset is 7344087\n",
      "The number of rows in the second transformed dataset is 559668\n",
      "The number of rows in the third transformed dataset is 30416\n",
      "The number of rows in the final dataset is 28557\n"
     ]
    }
   ],
   "source": [
    "# UNDERSTANDING TRANSFORMATIONS\n",
    "print(f'The number of rows in the original dataset is {len(all_train_services)}')\n",
    "print(f'The number of rows in the cleaned dataset is {len(ns_train_data)}')\n",
    "print(f'The number of rows in the first transformed dataset is {len(definite_df)}')\n",
    "print(f'The number of rows in the second transformed dataset is {len(trajectories_per_day)}')\n",
    "print(f'The number of rows in the third transformed dataset is {len(monthly_trajectories)}')\n",
    "print(f'The number of rows in the final dataset is {len(monthly_trajectories_nl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATASET AS CSV \n",
    "definite_df.to_csv('/Users/merelkamper/Documents/MSc Data Science/Thesis/MSc_thesis_code/data/first_transformation.csv', index=False)\n",
    "trajectories_per_day.to_csv('/Users/merelkamper/Documents/MSc Data Science/Thesis/MSc_thesis_code/data/daily_trajectories.csv', index=False)\n",
    "monthly_trajectories_nl.to_csv('/Users/merelkamper/Documents/MSc Data Science/Thesis/MSc_thesis_code/data/monthly_trajectories.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "551b8dbe5bdba8709305c769fb087bd28ca6e629e31012ae488442e2be74d2db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
